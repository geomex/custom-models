{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case I need to update datarobot-drum\n",
    "!pip install datarobot-drum --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/readmissions_train.csv')\n",
    "\n",
    "X = train.drop('readmitted',axis=1)\n",
    "X.drop(['diag_1_desc', 'diag_1', 'diag_2', 'diag_3'],axis=1,inplace=True)\n",
    "y = train.pop('readmitted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Preprocessing step per type of column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing for numerical features\n",
    "numeric_features = list(X.select_dtypes('int64').columns)\n",
    "for c in numeric_features:\n",
    "    X[c] = X[c].fillna(0)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "#Preprocessing for categorical features\n",
    "categorical_features = list(X.select_dtypes('object').columns)\n",
    "for c in categorical_features:\n",
    "    X[c] = X[c].fillna('missing')\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('OneHotEncoder', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "#Preprocessor with all of the steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full preprocessing pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "#Train the model-Pipeline\n",
    "pipeline.fit(X,y)\n",
    "\n",
    "#Preprocess x\n",
    "preprocessed = pipeline.transform(X)\n",
    "\n",
    "#I could also train the model with the sparse matrix. I transform it to padnas because the hook function in custom.py expected a pandas dataframe to be used for scoring.\n",
    "preprocessed = pd.DataFrame.sparse.from_spmatrix(preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train XGboost Classifier\n",
    "Normally, the XGboost classifier could be part of the final scikit-learn pipeline. I am opting to keep them separate in order to create a more complicated example with different pkl files for preprocessing and scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.2,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.02, max_delta_step=0, max_depth=10,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=300, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBClassifier(colsample_bylevel=0.2, max_depth= 10, learning_rate = 0.02, n_estimators=300)\n",
    "model.fit(preprocessed, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Custom Model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['custom_model/model.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipeline,'custom_model/preprocessing.pkl')\n",
    "joblib.dump(model, 'custom_model/model.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/joel.gongora/DataRobot/repos/FORKED/custom-models/drum_overview/mlops-examples/custom_inference/python/readmissions/Readmission_level_2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Validation checks results\n",
      "      Test case          Status   Details\n",
      "=========================================\n",
      "Basic batch prediction   PASSED          \n",
      "Null value imputation    PASSED          \n"
     ]
    }
   ],
   "source": [
    "!drum validation --code-dir ./custom_model --input ../data/readmissions_test.csv --target-type binary --positive-class-label True --negative-class-label False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate model can work as `Custom Training Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files were overwritten: {'/var/folders/v3/1cwf1zz90_nczrfzhvxwyjfr0000gp/T/tmpvnquru3s/model.pkl', '/var/folders/v3/1cwf1zz90_nczrfzhvxwyjfr0000gp/T/tmpvnquru3s/preprocessing.pkl'}\n",
      "Validation Complete ðŸŽ‰ Your model can be fit to your data,  and predictions can be made on the fit model! \n",
      " You're ready to add it to DataRobot. \n"
     ]
    }
   ],
   "source": [
    "!drum fit --code-dir ./custom_model --input ../data/readmissions_train.csv --target-type binary --target readmitted --positive-class-label True --negative-class-label False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         True     False\n",
      "0    0.532502  0.467498\n",
      "1    0.716724  0.283276\n",
      "2    0.655804  0.344196\n",
      "3    0.616853  0.383147\n",
      "4    0.816218  0.183782\n",
      "..        ...       ...\n",
      "495  0.586178  0.413822\n",
      "496  0.458597  0.541403\n",
      "497  0.466822  0.533178\n",
      "498  0.478825  0.521175\n",
      "499  0.349466  0.650534\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "!drum score --code-dir ./custom_model --input ../data/readmissions_test.csv --target-type binary --positive-class-label True --negative-class-label False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
